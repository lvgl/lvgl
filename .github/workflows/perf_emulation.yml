name: Emulated Performance Test

on:
  push:
    branches: 'master'
  pull_request:
    branches: 'master'

concurrency:
  group: ${{ github.ref }}-${{ github.workflow }} 
  cancel-in-progress: true

jobs:
  check_scripts:
    runs-on: ubuntu-24.04
    name: ARM Emulated Benchmark - Script Check
    strategy:
      fail-fast: false
      matrix: 
        test_script:
          - scripts/perf/tests/filter_docker_logs/test.sh
          # These scripts aren't executed in this workflow directly. Instead, they're run
          # by the `Emulated Performance Test Results Handler` workflow.
          # That workflow runs in the context of the `master` branch, so this workflow ensures
          # the scripts are tested in the PR context before merging. Helping to prevent CI failures in `master`
          - scripts/perf/tests/benchmark_results_comment/test.sh
          - scripts/perf/tests/serialize_results/test.sh
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check Script
        run: |
          pip3 install msgpack==1.1.0
          ./${{ matrix.test_script }}

  run_benchmark:
    runs-on: ubuntu-24.04
    name: ARM Emulated Benchmark ${{ matrix.image_type }} - ${{matrix.config }}
    needs: check_scripts
    strategy:
      fail-fast: false
      matrix:
        include:
        - image_type: 32b
          image_version : |
            main@sha256:0c4880924a58faa8a4326cd8e5a6fbeb892ce2cef69bbf34eb26201855fdc7c4
          config : lv_conf_perf32b
        - image_type: 64b
          image_version: |
            main@sha256:2609ccf298640570bc0303ba7716017c56fb93e8cae4a81f2d19df2219a6ffcc
          config : lv_conf_perf64b
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Generate lv_conf.h
        run: |
          cp lv_conf_template.h configs/ci/perf/lv_conf_perf.h
          python scripts/generate_lv_conf.py \
            --template lv_conf_template.h \
            --config configs/ci/perf/lv_conf_perf.h \
            --defaults configs/ci/perf/${{matrix.config}}.defaults
      - name: Run Benchmark Demo
        run: |
          # Retry logic is added here to work around rare loop device mount failures.
          # Occasionally, mounting the emulated partition fails due to a timing/race condition,
          # where the /dev/loopXpY device isn't fully initialized yet.
          # Retrying usually succeeds on the next attempt.
          attempts=0
          max_attempts=3

          while [ "$attempts" -lt "$max_attempts" ];
          do
            # Mounts the current lvgl source code into the container and runs the demo benchmark
            # To add additional build dependencies, modify the `lvperf_dependencies.sh` script
            docker run -t --privileged \
              --name so3-lvperf \
              -v /dev:/dev \
              -v $(pwd)/configs/ci/perf/lv_conf_perf.h:/so3/usr/lib/lv_conf.h \
              -v $(pwd):/so3/usr/lib/lvgl \
              -v $(pwd)/scripts/perf/lvperf_dependencies.sh:/so3/install_dependencies.sh \
              ghcr.io/smartobjectoriented/so3-lvperf${{matrix.image_type}}:${{matrix.image_version}}

            if [ $? -eq 0 ]; then
              break
            fi

            attempts=$((attempts+1))
            echo "Docker run failed (attempt $attempts/$max_attempts)."

            echo "--- dmesg output ---"
            sudo dmesg | tail -n 50

            if [ "$attempts" -lt "$max_attempts" ]; then
              echo "Retrying in 5 seconds..."
              sleep 5
            else
              echo "All attempts failed."
              exit 1
            fi
          done

      - name: Collect Logs
        run: |
          sudo cat $(docker inspect --format='{{.LogPath}}' so3-lvperf) | python scripts/perf/filter_docker_benchmark_logs.py results-${{matrix.image_type}}-${{matrix.config}}.json

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{matrix.image_type}}-${{matrix.config}}
          path: results-${{matrix.image_type}}-${{matrix.config}}.json
          if-no-files-found: error

  save_pr_number:
    runs-on: ubuntu-24.04
    if: ${{ github.event_name == 'pull_request' }}
    name: ARM Emulated Benchmark - Save PR Number
    needs: run_benchmark
    steps:
      - name: Save PR number 
        run: |
          echo ${{ github.event.number }} > pr_number

      - name: Upload PR number as artifact
        uses: actions/upload-artifact@v4
        with:
          name: pr_number
          path: pr_number
